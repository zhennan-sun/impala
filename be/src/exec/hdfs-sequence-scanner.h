// Copyright 2012 Cloudera Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.


#ifndef IMPALA_EXEC_HDFS_SEQUENCE_SCANNER_H
#define IMPALA_EXEC_HDFS_SEQUENCE_SCANNER_H

<<<<<<< HEAD
#include "util/codec.h"
#include "exec/hdfs-scanner.h"
#include "exec/delimited-text-parser.h"

namespace impala {

=======
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
// This scanner parses Sequence file located in HDFS, and writes the
// content as tuples in the Impala in-memory representation of data, e.g.
// (tuples, rows, row batches).
//
// TODO: Make the various sequence file formats behave more similarly.  They should
// all have a structure similar to block compressed operating in batches rather than
// row at a time.  
//
// org.apache.hadoop.io.SequenceFile is the original SequenceFile implementation
// and should be viewed as the canonical definition of this format. If
// anything is unclear in this file you should consult the code in
// org.apache.hadoop.io.SequenceFile.
//
// The following is a pseudo-BNF grammar for SequenceFile. Comments are prefixed
// with dashes:
//
// seqfile ::=
//   <file-header>
//   <record-block>+
//
// record-block ::=
//   <record>+
//   <file-sync-hash>
//
// file-header ::=
//   <file-version-header>
//   <file-key-class-name>
//   <file-value-class-name>
//   <file-is-compressed>
//   <file-is-block-compressed>
//   [<file-compression-codec-class>]
//   <file-header-metadata>
//   <file-sync-field>
//
// file-version-header ::= Byte[4] {'S', 'E', 'Q', 6}
//
// -- The name of the Java class responsible for reading the key buffer
//
// file-key-class-name ::=
//   Text {"org.apache.hadoop.io.BytesWritable"}
//
// -- The name of the Java class responsible for reading the value buffer
//
// -- We don't care what this is.
// file-value-class-name ::=
//
// -- Boolean variable indicating whether or not the file uses compression
// -- for key/values in this file
//
// file-is-compressed ::= Byte[1]
//
// -- A boolean field indicating whether or not the file is block compressed.
//
// file-is-block-compressed ::= Byte[1] {false}
//
// -- The Java class name of the compression codec iff <file-is-compressed>
// -- is true. The named class must implement
// -- org.apache.hadoop.io.compress.CompressionCodec.
// -- The expected value is org.apache.hadoop.io.compress.GzipCodec.
//
// file-compression-codec-class ::= Text
//
// -- A collection of key-value pairs defining metadata values for the
// -- file. The Map is serialized using standard JDK serialization, i.e.
// -- an Int corresponding to the number of key-value pairs, followed by
// -- Text key and value pairs.
//
// file-header-metadata ::= Map<Text, Text>
//
// -- A 16 byte marker that is generated by the writer. This marker appears
// -- at regular intervals at the beginning of records or record blocks
// -- intended to enable readers to skip to a random part of the file
// -- the sync hash is preceeded by a length of -1, refered to as the sync marker
//
// file-sync-hash ::= Byte[16]
//
// -- Records are all of one type as determined by the compression bits in the header
//
// record ::=
//   <uncompressed-record>     |
//   <block-compressed-record> |
//   <record-compressed-record>
//
// uncompressed-record ::=
//   <record-length>
//   <key-length>
//   <key>
//   <value>
//
// record-compressed-record ::=
//   <record-length>
//   <key-length>
//   <key>
//   <compressed-value>
//
<<<<<<< HEAD
// block-compessed-record ::=
=======
// block-compressed-record ::=
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
//   <file-sync-field>
//   <key-lengths-block-size>
//   <key-lengths-block>
//   <keys-block-size>
//   <keys-block>
//   <value-lengths-block-size>
//   <value-lengths-block>
//   <values-block-size>
//   <values-block>
//
// record-length := Int
// key-length := Int
// keys-lengths-block-size> := Int
// value-lengths-block-size> := Int
//
// keys-block :: = Byte[keys-block-size]
// values-block :: = Byte[values-block-size]
//
// -- The key-lengths and value-lengths blocks are are a sequence of lengths encoded
// -- in ZeroCompressedInteger (VInt) format.
//
// key-lengths-block :: = Byte[key-lengths-block-size]
// value-lengths-block :: = Byte[value-lengths-block-size]
//
// Byte ::= An eight-bit byte
//
// VInt ::= Variable length integer. The high-order bit of each byte
// indicates whether more bytes remain to be read. The low-order seven
// bits are appended as increasingly more significant bits in the
// resulting integer value.
//
// Int ::= A four-byte integer in big-endian format.
//
// Text ::= VInt, Chars (Length prefixed UTF-8 characters)

<<<<<<< HEAD
class HdfsSequenceScanner : public HdfsScanner {
=======
#include "exec/base-sequence-scanner.h"

namespace impala {

class DelimitedTextParser;

class HdfsSequenceScanner : public BaseSequenceScanner {
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
 public:
  // The four byte SeqFile version header present at the beginning of every
  // SeqFile file: {'S', 'E', 'Q', 6}
  static const uint8_t SEQFILE_VERSION_HEADER[4];

  HdfsSequenceScanner(HdfsScanNode* scan_node, RuntimeState* state);

  virtual ~HdfsSequenceScanner();
  
  // Implementation of HdfsScanner interface.
<<<<<<< HEAD
  virtual Status Prepare();
  virtual Status ProcessScanRange(ScanRangeContext* context);
  virtual Status Close();

  // Issue the initial scan ranges for all sequence files.
  static void IssueInitialRanges(HdfsScanNode*, const std::vector<HdfsFileDesc*>&);
  
  // Codegen writing tuples and evaluating predicates
  static llvm::Function* Codegen(HdfsScanNode*);

 private:
  // Sync indicator
  const static int SYNC_MARKER = -1;

  // Size of the sync hash field
  const static int SYNC_HASH_SIZE = 16;

  // Maximum size of a compressed block.  This is used to check for corrupted
  // block size so w do not read the whole file before we detect the error.
=======
  virtual Status Prepare(ScannerContext* context);

  // Codegen writing tuples and evaluating predicates.
  static llvm::Function* Codegen(HdfsScanNode*,
                                 const std::vector<ExprContext*>& conjunct_ctxs);

 protected:
  // Implementation of sequence container super class methods.
  virtual FileHeader* AllocateFileHeader();
  virtual Status ReadFileHeader();
  virtual Status InitNewRange();
  virtual Status ProcessRange();
  
  virtual THdfsFileFormat::type file_format() const { 
    return THdfsFileFormat::SEQUENCE_FILE; 
  }

 private:
  // Maximum size of a compressed block.  This is used to check for corrupted
  // block size so we do not read the whole file before we detect the error.
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
  const static int MAX_BLOCK_SIZE = (1024 * 1024 * 1024);

  // The value class name located in the SeqFile Header.
  // This is always "org.apache.hadoop.io.Text"
  static const char* const SEQFILE_VALUE_CLASS_NAME;

<<<<<<< HEAD
  // The key should always be 4 bytes.
  static const int SEQFILE_KEY_LENGTH;

  // Estimate of header size in bytes.  Headers are likely on remote nodes.  If
  // this is not big enough, the scanner will read more as necessary.
  static const int HEADER_SIZE;

  void IssueFileRanges(const char* filename);

  Status InitNewRange();
  Status ProcessRange();

  // Find the first record of a scan range.
  // If the scan range is not at the beginning of the file then this is called to
  // move the buffered_byte_stream_ seek point to before the next sync field.
  Status FindFirstRecord(bool* found);

  // Read the current Sequence file header from the begining of the file.
  // Verifies:
  //   version number
  //   key and data classes
  // Sets:
  //   is_compressed_
  //   is_blk_compressed_
  //   compression_codec_
  //   sync_
  Status ReadFileHeader();

  // Read the Sequence file Header Metadata section in the current file.
  // We don't use this information, so it is just skipped.
  Status ReadFileHeaderMetadata();

  // Read and validate a RowGroup sync field.
  Status ReadSync();

  // Read the record header, return if there was a sync block.
  // Sets:
  //   current_block_length_
  Status ReadBlockHeader(bool* sync);
=======
  // Read the record header.
  // Sets:
  //   current_block_length_
  Status ReadBlockHeader();
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa

  // Process an entire block compressed scan range.  Block compressed ranges are 
  // more common and can be parsed more efficiently in larger pieces.
  Status ProcessBlockCompressedScanRange();

<<<<<<< HEAD
  // Read a compressed block.
  // Decompress to unparsed_data_buffer_ allocated from unparsed_data_buffer_pool_.
  Status ReadCompressedBlock();

=======
  // Read a compressed block. Does NOT read sync or -1 marker preceding sync.
  // Decompress to unparsed_data_buffer_ allocated from unparsed_data_buffer_pool_.
  Status ReadCompressedBlock();

  // Utility function for parsing next_record_in_compressed_block_. Called by
  // ProcessBlockCompressedScanRange.
  Status ProcessDecompressedBlock();

>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
  // Read compressed or uncompressed records from the byte stream into memory
  // in unparsed_data_buffer_pool_.  Not used for block compressed files.
  // Output:
  //   record_ptr: ponter to the record.
  //   record_len: length of the record
<<<<<<< HEAD
  //   eors: set to true if we are at the end of the scan range.
  Status GetRecord(uint8_t** record_ptr, int64_t *record_len, bool* eosr);

  // read and verify a sync block.
  Status CheckSync();

  // Find the next sync block to start a scan range or recover from error.
  // IF the sync block spanned a buffer then set have_sync_ = true.
  Status SkipToSync();

  // Appends the current file and line to the RuntimeState's error log.
  // row_idx is 0-based (in current batch) where the parse error occured.
  virtual void LogRowParseError(std::stringstream*, int row_idx);
=======
  Status GetRecord(uint8_t** record_ptr, int64_t *record_len);
  
  // Appends the current file and line to the RuntimeState's error log.
  // row_idx is 0-based (in current batch) where the parse error occurred.
  virtual void LogRowParseError(int row_idx, std::stringstream*);
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
  
  // Helper class for picking fields and rows from delimited text.
  boost::scoped_ptr<DelimitedTextParser> delimited_text_parser_;
  std::vector<FieldLocation> field_locations_;

  // Data that is fixed across headers.  This struct is shared between scan ranges.
<<<<<<< HEAD
  struct FileHeader {
    // The sync hash read in from the file header.
    uint8_t sync[SYNC_HASH_SIZE];

    // File compression or not.
    bool is_compressed;
    // Block compression or not.
    bool is_blk_compressed;

    // Codec name if it is compressed.
    std::string codec;
  
    // End of the header block so we don't have to reparse it.
    int64_t header_size;
=======
  struct SeqFileHeader : public BaseSequenceScanner::FileHeader {
    // If true, the file uses row compression
    bool is_row_compressed;
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
  };

  // Struct for record locations and lens in compressed blocks.  
  struct RecordLocation {
    uint8_t* record;
    int64_t len;
  };

  // Records are processed in batches.  This vector stores batches of record locations
<<<<<<< HEAD
  // that are being processed.  Currently this is only used for block compression.
  // TODO: no reason we can't do this for record compressed/uncompressed.  
  // TODO: better perf not to use vector?
  std::vector<RecordLocation> record_locations_;

  // If true, this scanner is only processing the header bytes.
  bool only_parsing_header_;

  // Header for this scan range.  Memory is owned by the parent scan node.
  FileHeader* header_;

  // The decompressor class to use.
  boost::scoped_ptr<Codec> decompressor_;

  // Length of the current sequence file block (or record).
  int current_block_length_;

  // Length of the current key.  This should always be SEQFILE_KEY_LENGTH.
  int current_key_length_;

  // Pool for allocating the unparsed_data_buffer_.
  boost::scoped_ptr<MemPool> unparsed_data_buffer_pool_;

=======
  // that are being processed.  
  // TODO: better perf not to use vector?
  std::vector<RecordLocation> record_locations_;

  // Length of the current sequence file block (or record).
  int current_block_length_;

  // Length of the current key.  This is specified as 4 bytes in the format description.
  int current_key_length_;

>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
  // Buffer for data read from HDFS or from decompressing the HDFS data.
  uint8_t* unparsed_data_buffer_;

  // Number of buffered records unparsed_data_buffer_ from block compressed data.
  int64_t num_buffered_records_in_compressed_block_;

  // Next record from block compressed data.
  uint8_t* next_record_in_compressed_block_;
<<<<<<< HEAD

  // If we skip ahead on error and read the sync block this is set to true
  // so we do not need to look for it in ReadCompressedBlock.
  bool have_sync_;

  // Record the offset in the file of the start of a block or record
  // so we can point at it if there is a format error.
  int block_start_;
  
  // Time spent decompressing bytes
  RuntimeProfile::Counter* decompress_timer_;
=======
>>>>>>> d520a9cdea2fc97e8d5da9fbb0244e60ee416bfa
};

} // namespace impala

#endif // IMPALA_EXEC_HDFS_SEQUENCE_SCANNER_H
